{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjxxkxx/SeSAC/blob/main/Deep_learning_%26_pytorch_introduction%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch 설치 및 확인\n",
        "\n"
      ],
      "metadata": {
        "id": "7R1ybKJ7JUK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYC7h8bMATYj",
        "outputId": "1b55e12b-e42b-42bb-a5ac-b12ecdef5b37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "v_C7Xg6KJjm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a7c8ec-4b86-42de-8fcd-0d24517ee93e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Tensor\n",
        "\n",
        "pytorch의 가장 근본이 되는 Tensor들에 대해서 배워보겠습니다.\n",
        "\n",
        "### Tensor 만드는 법\n",
        "\n",
        "\n",
        "torch.tensor(data): data는 튜플, 리스트, numpy 배열 등등임.\n",
        "\n",
        "주요 속성들\n",
        "- dtype: 데이터 타입\n",
        "- device: gpu에 있는지, cpu에 있는지\n",
        "- requires_grad: 이게 True면 미분값을 계산함. 아니면 하지 않음.\n"
      ],
      "metadata": {
        "id": "WAIaC3rWAp3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0-D Tensor\n",
        "scalar = torch.tensor(5.0)\n",
        "number = torch.tensor(1.0)\n",
        "print(scalar)  # tensor(5.)\n",
        "print(number) # tensor(1.)"
      ],
      "metadata": {
        "id": "sZPgQV3KoVgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054e157f-9341-4878-febb-5f4d82d40eab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.)\n",
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0-D Tensor\n",
        "scalar = torch.tensor(5.0)\n",
        "number = torch.tensor(1.0)\n",
        "print(scalar)  # tensor(5.)\n",
        "print(number)  #tensor(1.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOILb3NGJrx",
        "outputId": "9e8bc700-872a-45e7-df26-4144df477871"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.)\n",
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = torch.tensor([1.0, 2.0, 3.0])\n",
        "tuple_vector = torch.tensor((1,2,3))\n",
        "\n",
        "print(vector)\n",
        "print(tuple_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9OyzomGpZg0",
        "outputId": "e159b010-6a71-4de5-9b45-0990e35eccec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "matrix2 = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
        "print(matrix.shape)\n",
        "print(matrix2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPPeYlefpXAh",
        "outputId": "4d2d1b5b-1578-4dfe-a9b4-f536cd700589"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix2 = [[1,2,3],[3,4,5]]\n",
        "lst = [matrix2, matrix2, matrix2, matrix2]\n",
        "\n",
        "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
        "tensor_3d_2 = torch.tensor(lst) # len(lst), len(lst[0]), len(lst[0][0])\n",
        "print(tensor_3d.shape)\n",
        "print(tensor_3d_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b68jvlvppkXU",
        "outputId": "4dd2def0-f893-4d83-f20d-68fe67512a36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 1])\n",
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensor의 주요 속성들\n",
        "\n",
        "- tensor.shape\n",
        "- tensor.size()\n",
        "- tensor.dtype"
      ],
      "metadata": {
        "id": "3cMM5BWTmTSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.shape)    # torch.Size([3])\n",
        "print(matrix.size())   # torch.Size([2, 2])\n",
        "print(tensor_3d.shape) # torch.Size([2, 2, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJg4nrPAMt7t",
        "outputId": "81d61a18-e27a-466d-979f-f06991ad68a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.dtype)    # torch.float32\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "print(int_tensor.dtype)  # torch.int32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "print(tensor_gpu.device)  # cuda:0 or cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrqNxnpxMwE3",
        "outputId": "f8c8a5b2-dc17-4c3d-97ec-78edc534683e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.int32\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensor 만드는 방법\n",
        "\n",
        "- torch.tensor(data)\n",
        "- 자주 쓰는 텐서들은 만드는 함수가 있음.\n",
        "  * torch.zeros(size): size 형태로 된, 0으로 된 텐서를 만듬.\n",
        "  * torch.ones(size): size 형태로 된, 1로 된 텐서를 만듬.\n",
        "  * torch.rand(size) / torch.randn(size) : 랜덤한 숫자로 된 텐서를 만듬. rand는 0과 1 사이에서 랜덤하게, randn은 표준정규분포(평균 0, 표준편차 1)에서 뽑아옴.\n",
        "  * torch.eye(n): 대각선만 1이고 이외에는 0인 2D 텐서(행렬)을 만듬.\n",
        "- 이외에도 많이 쓰이는 함수들\n",
        "  * torch.arange: range() 함수와 매우 비슷하다.\n",
        "  * torch.linspace(start, end, steps): start부터, end까지, steps개의 숫자를 가지는 텐서를 만듬. 이 때, 숫자들은 등간격으로 만들어짐.\n"
      ],
      "metadata": {
        "id": "_GXHR4BemnJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_tensor = torch.tensor([1, 2, 3])\n",
        "tuple_tensor = torch.tensor((4, 5, 6))\n",
        "\n",
        "zeros = torch.zeros((2, 3))\n",
        "ones = torch.ones((2, 3))\n",
        "rand = torch.rand((2, 3))\n",
        "eye = torch.eye(3)  # 3x3 Identity matrix\n",
        "print(zeros)\n",
        "print(ones)\n",
        "print(rand)\n",
        "print(eye)\n",
        "\n",
        "normal = torch.randn((2, 3))  # Normal distribution\n",
        "\n",
        "arange_tensor = torch.arange(start=0, end=10, step=2) # range(0, 10. 2)\n",
        "linspace_tensor = torch.linspace(start=0, end=1, steps=5) # 0 0.25 0.5 0.75 1\n",
        "print(arange_tensor)\n",
        "print(linspace_tensor)\n",
        "\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)"
      ],
      "metadata": {
        "id": "HuUTrq-ekUmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f59c05a-9955-4c4b-b703-165d37587e5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.0781, 0.5872, 0.7268],\n",
            "        [0.5073, 0.3363, 0.8781]])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "tensor([0, 2, 4, 6, 8])\n",
            "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: 위 각 함수들을 torch.tensor와 파이썬 리스트 operation들을 이용하여 재구현해 보세요."
      ],
      "metadata": {
        "id": "k6Pf1-B3vOVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "import random\n",
        "\n",
        "def nested_list(shape, value = 0):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [value for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [nested_list(shape[1:], value = value) for _ in range(l)]\n",
        "\n",
        "def random_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.random() for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:]) for _ in range(l)]\n",
        "\n",
        "def randomn_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.gauss(0, 1) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [randomn_nested_list(shape[1:]) for _ in range(l)]\n",
        "\n",
        "def zeros(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 0))\n",
        "\n",
        "def ones(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 1))\n",
        "\n",
        "def rand(shape):\n",
        "    return torch.tensor(random_nested_list(shape))\n",
        "\n",
        "def randn(shape):\n",
        "    return torch.tensor(randomn_nested_list(shape))\n",
        "\n",
        "print(nested_list((2, 3, 4), value = 0))\n",
        "print(zeros((2,3,4)))\n",
        "print(ones((2,3,4)))\n",
        "print(rand((2,3,4)))\n",
        "\n",
        "def eyes(n):\n",
        "    lst = [[0 for i in range(n) for j in range(n)]]\n",
        "\n",
        "    lst = []\n",
        "    for i in range(n):\n",
        "        tmp = []\n",
        "        for j in range(n):\n",
        "            tmp.append(0)\n",
        "        lst.append(tmp)\n",
        "\n",
        "    for i in range(n):  # 대각선\n",
        "        lst[i][i] = 1\n",
        "\n",
        "    return torch.tensor(lst)"
      ],
      "metadata": {
        "id": "mUiK8UuXpHfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666d9a36-2b7b-411f-ffaf-80f5de5c29f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n",
            "tensor([[[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]]])\n",
            "tensor([[[1, 1, 1, 1],\n",
            "         [1, 1, 1, 1],\n",
            "         [1, 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1, 1],\n",
            "         [1, 1, 1, 1],\n",
            "         [1, 1, 1, 1]]])\n",
            "tensor([[[0.1591, 0.8194, 0.4731, 0.3345],\n",
            "         [0.2443, 0.2277, 0.8575, 0.8895],\n",
            "         [0.9861, 0.1065, 0.5945, 0.9292]],\n",
            "\n",
            "        [[0.3164, 0.9687, 0.8571, 0.4492],\n",
            "         [0.3716, 0.5599, 0.1990, 0.7955],\n",
            "         [0.8698, 0.0987, 0.2315, 0.7571]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensor끼리의 연산\n",
        "\n",
        "일반적인 사칙연산, 행렬 곱(matmul), 원소간 곱 등등이 다 적용됨."
      ],
      "metadata": {
        "id": "PWpAlToLo7wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "add = a + b  # tensor([5., 7., 9.])\n",
        "sub = a - b  # tensor([-3., -3., -3.])\n",
        "\n",
        "mul = a * b  # tensor([ 4., 10., 18.])\n",
        "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
        "\n",
        "exp = a ** 2  # tensor([1., 4., 9.])"
      ],
      "metadata": {
        "id": "JsvDLKcKkd2e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "'''\n",
        "1 2  5 6\n",
        "3 4  7 8\n",
        "\n",
        "1 3\n",
        "2 4\n",
        "'''\n",
        "'''\n",
        "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
        "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
        "'''\n",
        "\n",
        "matmul = torch.matmul(matrix_a, matrix_b)\n",
        "\n",
        "# tensor([[19, 22],\n",
        "#         [43, 50]])\n",
        "\n",
        "elem_mul = matrix_a * matrix_b\n",
        "# tensor([[ 5, 12],\n",
        "#         [21, 32]])\n",
        "\n",
        "transposed = torch.transpose(matrix_a, 0, 1)\n",
        "# tensor([[1, 3],\n",
        "#         [2, 4]])\n",
        "\n",
        "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))"
      ],
      "metadata": {
        "id": "eAa8J66qkjhp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadcasting\n",
        "\n",
        "브로드캐스팅은 서로 다른 크기를 가진 텐서들 간에 연산을 수행할 때, 자동으로 크기를 맞춰주는 PyTorch(및 NumPy)의 기능입니다. 이 기능은 명시적으로 텐서의 크기를 변환하지 않아도, 작은 크기의 텐서를 큰 크기의 텐서와 함께 연산할 수 있도록 해줍니다. Pandas나 Numpy 등에서도 자주 활용되기 때문에 알아두면 좋습니다.\n",
        "\n",
        "브로드캐스팅 규칙:\n",
        "1. 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때, 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "2. 크기 맞추기: 각 차원에서 크기가 1인 텐서는 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "3. 불가능한 경우: 두 텐서가 특정 차원에서 서로 다른 크기를 가지며, 그중 하나가 1이 아니면 브로드캐스팅이 불가능하고 오류가 발생합니다.\n",
        "\n",
        "예를 들어서,\n",
        "\n",
        "- (2,3) 크기의 텐서에 (3,) 크기의 텐서를 더하면, (2,3) 크기의 텐서가 됩니다. 이 때 (3,) 크기의 텐서들은 첫 번째 차원에 대해서 다 더해집니다.\n"
      ],
      "metadata": {
        "id": "Z5Mjhn93pEQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])   # Shape: (2, 3)\n",
        "# b = torch.tensor([1, 2, 3])                # Shape: (3,)\n",
        "# b = torch.tensor([[1, 2, 3]])              # Shape: (1, 3)\n",
        "b = torch.tensor([[1, 2, 3], [1, 2, 3]])   # Shape: (2, 3)\n",
        "\n",
        "broadcast_add = a + b  # Shape: (2, 3)\n",
        "# tensor([[2, 4, 6],\n",
        "#         [5, 7, 9]])\n",
        "\n",
        "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
        "b = torch.tensor([4, 5, 6])        # Shape: (3)\n",
        "\n",
        "b = torch.tensor([[4, 5, 6]])        # Shape: (1,3)\n",
        "\n",
        "b = torch.tensor([[4, 5, 6],[4, 5, 6],[4, 5, 6]])        # Shape: (3,3)\n",
        "a = torch.tensor([[1, 1, 1], [2, 2, 3], [3, 3, 3]])  # Shape: (3, 1)\n",
        "\n",
        "a*b = [[4, 5, 6], [8, 10, 18], [12, 15, 18]]\n",
        "# To make shapes compatible:\n",
        "# a: (3, 1) -> (3, 3)\n",
        "# b: (3,)   -> (1, 3) -> (3, 3)\n",
        "\n",
        "broadcast_mul = a * b  # Shape: (3, 3)\n",
        "# tensor([[ 4,  5,  6],\n",
        "#         [ 8, 10, 12],\n",
        "#         [12, 15, 18]])"
      ],
      "metadata": {
        "id": "VolcxkK_krYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이 외 tensor operation들\n",
        "\n",
        "- Slicing / Indexing\n",
        "- Reshaping\n",
        "- Concatenation / Stacking"
      ],
      "metadata": {
        "id": "xN5jjYflr1oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slicing / indexing\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Basic indexing\n",
        "element = tensor[1, 2]  # tensor(6)\n",
        "\n",
        "# Slicing\n",
        "sub_tensor = tensor[:, 1:]  # tensor([[2, 3],\n",
        "                            #         [5, 6],\n",
        "                            #         [8, 9]])\n",
        "\n",
        "# Advanced indexing with masks\n",
        "mask = tensor > 5\n",
        "filtered = tensor[mask]  # tensor([6, 7, 8, 9])"
      ],
      "metadata": {
        "id": "Jz0GLy02kt-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshaping\n",
        "\n",
        "tensor = torch.arange(0, 12)\n",
        "reshaped_view = tensor.view(3, 4)  # tensor([[ 0,  1,  2,  3],\n",
        "                                   #         [ 4,  5,  6,  7],\n",
        "                                   #         [ 8,  9, 10, 11]])\n",
        "\n",
        "reshaped_reshape = tensor.reshape(2, 6)  # tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "                                         #         [ 6,  7,  8,  9, 10, 11]])\n",
        "\n",
        "# tensor.permute\n",
        "tensor = torch.randn(2, 3, 4)\n",
        "permuted = tensor.permute(2, 0, 1)  # Changes the order of dimensions\n",
        "print(permuted.shape)  # torch.Size([4, 2, 3])\n"
      ],
      "metadata": {
        "id": "UiCkGFvjsnBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Concatenate along existing dimension\n",
        "concat = torch.cat((a, b), dim=0)  # tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Stack along a new dimension\n",
        "stack = torch.stack((a, b), dim=0)\n",
        "# tensor([[1, 2, 3],\n",
        "#         [4, 5, 6]])\n"
      ],
      "metadata": {
        "id": "VIHxbxPTs4Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수학적 함수들\n",
        "\n",
        "- abs, sqrt, exp, log 등 unary 함수들 (텐서 하나만을 input으로 받음): torch.abs, torch.sqrt, torch.exp, torch.log\n",
        "- max, min 등 binary 함수들 (텐서 2개를 input으로 받음): torch.max, torch.min\n",
        "- 차원을 하나 혹은 여럿 낮추는 Reduction Operation들: torch.sum(tensor, dim = n)\n"
      ],
      "metadata": {
        "id": "YCXr7r3Kr__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([-1.0, -2.0, 3.0])\n",
        "\n",
        "abs_a = torch.abs(a)          # tensor([1., 2., 3.])\n",
        "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
        "exp_a = torch.exp(a)          # tensor([0.3679, 0.1353, 20.0855])\n",
        "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])\n"
      ],
      "metadata": {
        "id": "UsFdYgJws8kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
        "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])\n"
      ],
      "metadata": {
        "id": "EYIgfoa7s-gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "sum_all = torch.sum(tensor)          # tensor(10)\n",
        "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6])\n",
        "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([3, 7])\n",
        "\n",
        "mean_all = torch.mean(tensor.float())  # tensor(2.5000)"
      ],
      "metadata": {
        "id": "lmMzGqgOs_kk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([2, 2, 2])\n",
        "\n",
        "greater = a > b  # tensor([False, False, True])\n",
        "equal = a == b   # tensor([False, True, False])\n"
      ],
      "metadata": {
        "id": "dLsS1TKhtAn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch로 다시 해 보는 선형회귀\n",
        "\n",
        "주어진 데이터 $(x_i, y_i)$ 에 대해서 $y=wx+b$에서, 가장 적절한 w와 b를 찾는 것이 선형회귀였음.\n",
        "\n",
        "y = wx + b 에서, w와 b는 parameter이고 x는 입력, y는 출력임.\n",
        "이 때 w랑 b를 구하기 위해서, 다음의 loss function을 최소화하는 방향으로 학습하고 싶다고 하자.\n",
        "\n",
        "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "\n",
        "원래는 저 값을 그냥 바로 식으로 계산할 수 있었지만, 언제나 그렇지는 않기 때문에 (선형회귀 외의 다른 모델들에서) 수치적으로 계산해보자.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wo5udXG_t6Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# y[0]= true_w * X[0] + true_b + torch.randn(1, 1) * 2\n",
        "# y[1]= true_w * X[1] + true_b + torch.randn(1, 1) * 2\n",
        "# ...\n",
        "# y[99]\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "learning_rate = 0.005\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters using gradient descent\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Zero gradients after updating\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeaB0-MIwM3h",
        "outputId": "45447762-89bf-4b28-d390-c732ae2db2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: w = 2.2849, b = -1.4679, loss = 777.7333\n",
            "Epoch 100: w = 2.0032, b = 0.1330, loss = 3.9277\n",
            "Epoch 200: w = 2.0074, b = 0.7215, loss = 3.1670\n",
            "Epoch 300: w = 2.0089, b = 0.9381, loss = 3.0639\n",
            "Epoch 400: w = 2.0095, b = 1.0178, loss = 3.0500\n",
            "Epoch 500: w = 2.0097, b = 1.0472, loss = 3.0481\n",
            "Epoch 600: w = 2.0097, b = 1.0580, loss = 3.0478\n",
            "Epoch 700: w = 2.0098, b = 1.0619, loss = 3.0478\n",
            "Epoch 800: w = 2.0098, b = 1.0634, loss = 3.0478\n",
            "Epoch 900: w = 2.0098, b = 1.0639, loss = 3.0478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: 위 선형회귀 부분을 함수로 만들고, 다양한 하이퍼파라미터 (여기서의 hyperparameter은 learning rate 뿐임)를 바꿔가며 최적의 모델을 찾아보세요.\n"
      ],
      "metadata": {
        "id": "Y2IbgzEsuXLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here"
      ],
      "metadata": {
        "id": "dDFD6NNUuW5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 선형회귀 조금 더 해보기\n"
      ],
      "metadata": {
        "id": "XlzOMd44rinI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: 이번에는 비슷하게, 입력이 3개이고 출력이 1개인 선형회귀를 해 보자.\n",
        "\n",
        "$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$"
      ],
      "metadata": {
        "id": "pgue321swjEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n"
      ],
      "metadata": {
        "id": "Pe5Uqs91xjtC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "cee17ffe-63b4-4764-f0d6-5aa7aa01c70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-386a2e519ef8>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrue_w2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrue_w3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrue_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_w1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_w2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_w3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrue_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 뒤에서 할 내용 미리 살짝 엿보기 - Optimizer\n"
      ],
      "metadata": {
        "id": "ROZ96vR1wI5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이번에는 adam optimizer를 한번 사용해보자.\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 아담 옵티마이저 하이퍼파라미터 설정\n",
        "learning_rate = 0.005\n",
        "epochs = 10000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
        "m_w = torch.zeros_like(w)\n",
        "v_w = torch.zeros_like(w)\n",
        "m_b = torch.zeros_like(b)\n",
        "v_b = torch.zeros_like(b)\n",
        "\n",
        "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
        "t = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 아담 옵티마이저 업데이트\n",
        "    with torch.no_grad():\n",
        "        t += 1  # 시간 스텝 증가\n",
        "\n",
        "        # w 파라미터 업데이트\n",
        "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_w_hat = m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # b 파라미터 업데이트\n",
        "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_b_hat = m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    # Gradients 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR7URmNB0e7n",
        "outputId": "84328edb-722a-45bf-e4e1-1354937da925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: w = 0.0580, b = 0.1627, loss = 364.6054\n",
            "Epoch 200: w = 0.4848, b = 0.5600, loss = 224.0036\n",
            "Epoch 300: w = 0.8526, b = 0.8696, loss = 130.7948\n",
            "Epoch 400: w = 1.1600, b = 1.0872, loss = 72.6629\n",
            "Epoch 500: w = 1.4081, b = 1.2152, loss = 38.8180\n",
            "Epoch 600: w = 1.6004, b = 1.2650, loss = 20.5415\n",
            "Epoch 700: w = 1.7433, b = 1.2549, loss = 11.4278\n",
            "Epoch 800: w = 1.8446, b = 1.2065, loss = 7.2398\n",
            "Epoch 900: w = 1.9132, b = 1.1396, loss = 5.4651\n",
            "Epoch 1000: w = 1.9575, b = 1.0696, loss = 4.7695\n",
            "Epoch 1100: w = 1.9848, b = 1.0066, loss = 4.5158\n",
            "Epoch 1200: w = 2.0009, b = 0.9553, loss = 4.4291\n",
            "Epoch 1300: w = 2.0100, b = 0.9169, loss = 4.4010\n",
            "Epoch 1400: w = 2.0149, b = 0.8900, loss = 4.3923\n",
            "Epoch 1500: w = 2.0174, b = 0.8725, loss = 4.3898\n",
            "Epoch 1600: w = 2.0187, b = 0.8617, loss = 4.3890\n",
            "Epoch 1700: w = 2.0193, b = 0.8554, loss = 4.3889\n",
            "Epoch 1800: w = 2.0195, b = 0.8519, loss = 4.3888\n",
            "Epoch 1900: w = 2.0197, b = 0.8502, loss = 4.3888\n",
            "Epoch 2000: w = 2.0197, b = 0.8493, loss = 4.3888\n",
            "Epoch 2100: w = 2.0197, b = 0.8489, loss = 4.3888\n",
            "Epoch 2200: w = 2.0197, b = 0.8487, loss = 4.3888\n",
            "Epoch 2300: w = 2.0197, b = 0.8487, loss = 4.3888\n",
            "Epoch 2400: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 2500: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 2600: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 2700: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 2800: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 2900: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3000: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3100: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3200: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3300: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3400: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3500: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3600: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3700: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3800: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 3900: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4000: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4100: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4200: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4300: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4400: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4500: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4600: w = 2.0197, b = 0.8486, loss = 4.3888\n",
            "Epoch 4700: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 4800: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 4900: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5000: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5100: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5200: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5300: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5400: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5500: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5600: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5700: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5800: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 5900: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6000: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6100: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6200: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6300: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6400: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6500: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6600: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6700: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6800: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 6900: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7000: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7100: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7200: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7300: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7400: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7500: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7600: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7700: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7800: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 7900: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8000: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8100: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8200: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8300: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8400: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8500: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8600: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8700: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8800: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 8900: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9000: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9100: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9200: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9300: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9400: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9500: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9600: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9700: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9800: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 9900: w = 2.0198, b = 0.8486, loss = 4.3888\n",
            "Epoch 10000: w = 2.0198, b = 0.8486, loss = 4.3888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자꾸 local minima 어딘가에 빠지는 것 같다. 이걸 수정하기 위해서, 일정 횟수 이상 바뀌지 않으면 noise를 주는 방식을 생각해보자.\n"
      ],
      "metadata": {
        "id": "BhIwbEiQ6meH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "torch.manual_seed(42)  # 재현성을 위해 시드 설정\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 아담 옵티마이저 하이퍼파라미터 설정\n",
        "learning_rate = 0.005\n",
        "epochs = 10000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
        "m_w = torch.zeros_like(w)\n",
        "v_w = torch.zeros_like(w)\n",
        "m_b = torch.zeros_like(b)\n",
        "v_b = torch.zeros_like(b)\n",
        "\n",
        "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
        "t = 0\n",
        "\n",
        "\n",
        "patience = 300  # 손실과 파라미터 변화가 임계값 이하로 유지되는 에포크 수\n",
        "threshold_loss = 1e-4  # 손실 변화 임계값\n",
        "threshold_w = 1e-4     # w 변화 임계값\n",
        "threshold_b = 1e-4     # b 변화 임계값\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "w_history = []\n",
        "b_history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 아담 옵티마이저 업데이트\n",
        "    with torch.no_grad():\n",
        "        t += 1  # 시간 스텝 증가\n",
        "\n",
        "        # w 파라미터 업데이트\n",
        "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_w_hat = m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # b 파라미터 업데이트\n",
        "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_b_hat = m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    # Gradients 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # 손실과 파라미터 값을 기록\n",
        "    loss_history.append(loss.item())\n",
        "    w_history.append(w.item())\n",
        "    b_history.append(b.item())\n",
        "\n",
        "    # Patience에 도달했는지 확인\n",
        "    if epoch >= patience :\n",
        "        # 최근 'patience' 에포크의 손실 변화 계산\n",
        "        recent_losses = loss_history[-patience:]\n",
        "        loss_deltas = [abs(recent_losses[i] - recent_losses[i-1]) for i in range(1, patience)]\n",
        "        max_loss_delta = max(loss_deltas)\n",
        "\n",
        "        # 최근 'patience' 에포크의 w 변화 계산\n",
        "        recent_ws = w_history[-patience:]\n",
        "        w_deltas = [abs(recent_ws[i] - recent_ws[i-1]) for i in range(1, patience)]\n",
        "        max_w_delta = max(w_deltas)\n",
        "\n",
        "        # 최근 'patience' 에포크의 b 변화 계산\n",
        "        recent_bs = b_history[-patience:]\n",
        "        b_deltas = [abs(recent_bs[i] - recent_bs[i-1]) for i in range(1, patience)]\n",
        "        max_b_delta = max(b_deltas)\n",
        "\n",
        "        # 변화가 모두 임계값 이하인 경우 노이즈 추가\n",
        "        if (max_loss_delta < threshold_loss) and (max_w_delta < threshold_w) and (max_b_delta < threshold_b):\n",
        "            print(f'\\nEpoch {epoch}: No significant updates in the last {patience} epochs. Adding noise to parameters.')\n",
        "            # 파라미터에 노이즈 추가\n",
        "            noise_w = torch.randn_like(w) * 0.1\n",
        "            noise_b = torch.randn_like(b) * 0.1\n",
        "            w.data += noise_w\n",
        "            b.data += noise_b\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "\n",
        "# 최종 파라미터 출력\n",
        "print(f'\\nFinal Parameters: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-JPhYZV4PXg",
        "outputId": "dc154f03-4847-471f-c6a2-0d8f8446cd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: w = 0.8642, b = 0.2516, loss = 131.1085\n",
            "Epoch 200: w = 1.2482, b = 0.6152, loss = 59.2300\n",
            "Epoch 300: w = 1.5384, b = 0.8670, loss = 24.3059\n",
            "Epoch 400: w = 1.7397, b = 1.0158, loss = 9.8937\n",
            "Epoch 500: w = 1.8663, b = 1.0852, loss = 4.9355\n",
            "Epoch 600: w = 1.9380, b = 1.1053, loss = 3.5270\n",
            "Epoch 700: w = 1.9746, b = 1.1020, loss = 3.1972\n",
            "Epoch 800: w = 1.9914, b = 1.0920, loss = 3.1333\n",
            "Epoch 900: w = 1.9985, b = 1.0830, loss = 3.1230\n",
            "Epoch 1000: w = 2.0011, b = 1.0771, loss = 3.1216\n",
            "Epoch 1100: w = 2.0020, b = 1.0738, loss = 3.1215\n",
            "\n",
            "Epoch 1143: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 1200: w = 2.0690, b = 1.0295, loss = 3.5617\n",
            "Epoch 1300: w = 2.0176, b = 1.0432, loss = 3.1449\n",
            "Epoch 1400: w = 2.0055, b = 1.0597, loss = 3.1225\n",
            "Epoch 1500: w = 2.0029, b = 1.0675, loss = 3.1215\n",
            "Epoch 1600: w = 2.0025, b = 1.0703, loss = 3.1214\n",
            "Epoch 1700: w = 2.0024, b = 1.0711, loss = 3.1214\n",
            "\n",
            "Epoch 1719: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 1800: w = 1.9932, b = 1.0648, loss = 3.1300\n",
            "Epoch 1900: w = 2.0016, b = 1.0719, loss = 3.1215\n",
            "Epoch 2000: w = 2.0023, b = 1.0716, loss = 3.1214\n",
            "Epoch 2100: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "\n",
            "Epoch 2132: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 2200: w = 1.9996, b = 1.1031, loss = 3.1232\n",
            "Epoch 2300: w = 2.0022, b = 1.0747, loss = 3.1215\n",
            "Epoch 2400: w = 2.0024, b = 1.0716, loss = 3.1214\n",
            "Epoch 2500: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "\n",
            "Epoch 2589: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 2600: w = 2.0541, b = 0.9360, loss = 3.4015\n",
            "Epoch 2700: w = 2.0030, b = 1.0600, loss = 3.1216\n",
            "Epoch 2800: w = 2.0024, b = 1.0710, loss = 3.1214\n",
            "Epoch 2900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 3040: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 3100: w = 1.9999, b = 1.1043, loss = 3.1232\n",
            "Epoch 3200: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "Epoch 3300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 3459: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 3500: w = 2.0024, b = 1.0742, loss = 3.1215\n",
            "Epoch 3600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 3811: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 3900: w = 2.0015, b = 1.0751, loss = 3.1216\n",
            "Epoch 4000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 4227: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 4300: w = 2.0040, b = 1.0679, loss = 3.1218\n",
            "Epoch 4400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 4635: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 4700: w = 1.9985, b = 1.0766, loss = 3.1234\n",
            "Epoch 4800: w = 2.0024, b = 1.0715, loss = 3.1214\n",
            "Epoch 4900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 5000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 5044: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 5100: w = 2.0087, b = 1.0583, loss = 3.1268\n",
            "Epoch 5200: w = 2.0023, b = 1.0712, loss = 3.1214\n",
            "Epoch 5300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 5400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 5467: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 5500: w = 2.0052, b = 1.0815, loss = 3.1223\n",
            "Epoch 5600: w = 2.0023, b = 1.0713, loss = 3.1214\n",
            "Epoch 5700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 5800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 5830: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 5900: w = 2.0051, b = 1.0725, loss = 3.1224\n",
            "Epoch 6000: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "Epoch 6100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 6200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 6236: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 6300: w = 2.0013, b = 1.0746, loss = 3.1216\n",
            "Epoch 6400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 6500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 6600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 6627: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 6700: w = 2.0038, b = 1.0747, loss = 3.1217\n",
            "Epoch 6800: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "Epoch 6900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 7034: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 7100: w = 2.0048, b = 1.0767, loss = 3.1220\n",
            "Epoch 7200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 7453: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 7500: w = 1.9919, b = 1.0830, loss = 3.1343\n",
            "Epoch 7600: w = 2.0024, b = 1.0716, loss = 3.1214\n",
            "Epoch 7700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 7885: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 7900: w = 1.9786, b = 1.0225, loss = 3.1576\n",
            "Epoch 8000: w = 2.0025, b = 1.0722, loss = 3.1214\n",
            "Epoch 8100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 8200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 8295: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 8300: w = 2.0005, b = 1.0024, loss = 3.1283\n",
            "Epoch 8400: w = 2.0023, b = 1.0717, loss = 3.1214\n",
            "Epoch 8500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 8600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 8687: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 8700: w = 2.0347, b = 1.1205, loss = 3.1790\n",
            "Epoch 8800: w = 2.0022, b = 1.0708, loss = 3.1214\n",
            "Epoch 8900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 9103: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 9200: w = 2.0033, b = 1.0751, loss = 3.1215\n",
            "Epoch 9300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 9530: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 9600: w = 2.0013, b = 1.0695, loss = 3.1215\n",
            "Epoch 9700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 9914: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 10000: w = 2.0019, b = 1.0707, loss = 3.1215\n",
            "\n",
            "Final Parameters: w = 2.0019, b = 1.0707, loss = 3.1215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 딥러닝 들어가기\n",
        "\n",
        "\n",
        "아래 코드는 pytorch 에서 딥러닝 모델을 짤 때, 가장 일반적인 형식이라고 할 수 있다. 각 부분에서 쓰일 함수들은 문제에 따라서 다르지만, 대개의 경우 위 내용이 크게 바뀌지 않을 것임."
      ],
      "metadata": {
        "id": "ddcrBxNp7LCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "uUTgBrCI7k8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "input_features = 20\n",
        "\n",
        "# Features: random numbers\n",
        "X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "# Labels: sum of features > 0 => class 1, else class 0\n",
        "y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility\n",
        "\n",
        "# Create a dataset and data loader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "AuEAYWDf7h6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        self.relu = nn.ReLU()                         # ReLU activation\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = input_features\n",
        "hidden_size = 64\n",
        "output_size = 1  # Binary classification\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Combines a sigmoid layer and the BCELoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update weights\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
        "    predicted_classes = (predictions >= 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted_classes == y_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXe436j_7Uda",
        "outputId": "755e69c5-24e2-4a74-e012-cfdf1670df50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6249\n",
            "Epoch [2/20], Loss: 0.5482\n",
            "Epoch [3/20], Loss: 0.4171\n",
            "Epoch [4/20], Loss: 0.4820\n",
            "Epoch [5/20], Loss: 0.4052\n",
            "Epoch [6/20], Loss: 0.2625\n",
            "Epoch [7/20], Loss: 0.3147\n",
            "Epoch [8/20], Loss: 0.1852\n",
            "Epoch [9/20], Loss: 0.1286\n",
            "Epoch [10/20], Loss: 0.1301\n",
            "Epoch [11/20], Loss: 0.0808\n",
            "Epoch [12/20], Loss: 0.1353\n",
            "Epoch [13/20], Loss: 0.1846\n",
            "Epoch [14/20], Loss: 0.1222\n",
            "Epoch [15/20], Loss: 0.0643\n",
            "Epoch [16/20], Loss: 0.0972\n",
            "Epoch [17/20], Loss: 0.0886\n",
            "Epoch [18/20], Loss: 0.0872\n",
            "Epoch [19/20], Loss: 0.1103\n",
            "Epoch [20/20], Loss: 0.1142\n",
            "Accuracy: 99.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: input_size, hidden_size, learning_rate 등의 하이퍼파라미터를 바꿔 가며 최적의 모델을 찾아보세요."
      ],
      "metadata": {
        "id": "Vq5n0jb4vxWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here"
      ],
      "metadata": {
        "id": "6YxkejqAw8--"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}